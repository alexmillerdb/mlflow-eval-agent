# Evaluation Script Patterns

Templates and patterns for creating `run_eval.py` evaluation scripts.

---

## CRITICAL: MLflow Setup (Required)

Every evaluation script MUST include this setup before any MLflow calls:

```python
import mlflow

# REQUIRED: Set up MLflow tracking (must be in this exact order)
mlflow.set_tracking_uri("databricks")
mlflow.set_experiment(experiment_id="{experiment_id}")
```

**Why this order matters:**
- `set_tracking_uri("databricks")` configures authentication
- Without it, scripts fail in Databricks Serverless with credential errors
- This MUST come before `start_run()` or `genai.evaluate()`

---

## Template: Basic run_eval.py

```python
"""
Evaluation script for [Agent Name].
Generated by MLflow Eval Agent.
"""

import mlflow
from mlflow.genai.scorers import Guidelines, Safety, RelevanceToQuery

# Import local modules
from eval_dataset import get_eval_data
from scorers import get_scorers

# REQUIRED: MLflow setup (must be in this exact order)
mlflow.set_tracking_uri("databricks")
mlflow.set_experiment(experiment_id="{experiment_id}")


def run_evaluation():
    """Run evaluation on dataset with scorers."""
    # Load evaluation data and scorers
    eval_data = get_eval_data()
    scorers = get_scorers()

    print(f"Loaded {len(eval_data)} evaluation records")
    print(f"Using {len(scorers)} scorers")

    # Run evaluation
    with mlflow.start_run(run_name="eval_run"):
        results = mlflow.genai.evaluate(
            data=eval_data,
            scorers=scorers,
        )

    # Print results
    print(f"\nEvaluation complete!")
    print(f"Run ID: {results.run_id}")
    print(f"\nMetrics:")
    for metric, value in sorted(results.metrics.items()):
        if metric.endswith('/mean'):
            metric_name = metric.replace('/mean', '')
            print(f"  {metric_name}: {value:.2%}")

    return results


if __name__ == "__main__":
    run_evaluation()
```

---

## Template: With predict_fn

Use when dataset has `inputs` but no pre-computed `outputs`.

```python
"""
Evaluation script with predict function.
"""

import mlflow
from mlflow.genai.scorers import Guidelines, Safety

from eval_dataset import get_eval_data
from scorers import get_scorers

# REQUIRED: MLflow setup (must be in this exact order)
mlflow.set_tracking_uri("databricks")
mlflow.set_experiment(experiment_id="{experiment_id}")


def predict_fn(query, context=None, **kwargs):
    """
    Predict function - receives UNPACKED kwargs from inputs.

    If inputs = {"query": "What is X?", "context": "..."}
    Then predict_fn is called as: predict_fn(query="What is X?", context="...")

    NOTE: Signature uses **kwargs, NOT a dict!
    """
    # Your prediction logic here
    # Example: call your agent/model
    response = call_your_agent(query, context)
    return {"response": response}


def run_evaluation():
    """Run evaluation with predict function."""
    eval_data = get_eval_data()
    scorers = get_scorers()

    print(f"Loaded {len(eval_data)} evaluation records")
    print(f"Using {len(scorers)} scorers")

    with mlflow.start_run(run_name="eval_run"):
        results = mlflow.genai.evaluate(
            data=eval_data,
            predict_fn=predict_fn,  # Include when dataset has no outputs
            scorers=scorers,
        )

    print(f"\nRun ID: {results.run_id}")
    for metric, value in sorted(results.metrics.items()):
        if metric.endswith('/mean'):
            print(f"  {metric.replace('/mean', '')}: {value:.2%}")

    return results


if __name__ == "__main__":
    run_evaluation()
```

---

## Template: With Local Agent Import

Best practice for development - import agent directly.

```python
"""
Evaluation script using local agent import.
"""

import mlflow
from mlflow.genai.scorers import Guidelines, Safety

# Import agent directly (NOT via model serving endpoint)
from plan_execute_agent import AGENT

from eval_dataset import get_eval_data
from scorers import get_scorers

# REQUIRED: MLflow setup
mlflow.set_tracking_uri("databricks")
mlflow.set_experiment(experiment_id="{experiment_id}")

# Enable auto-tracing for the agent
mlflow.openai.autolog()


def predict_fn(messages):
    """Wrapper that calls the local agent directly."""
    result = AGENT.predict({"messages": messages})

    # Extract response from ResponsesAgent format
    if isinstance(result, dict) and "messages" in result:
        for msg in reversed(result["messages"]):
            if msg.get("role") == "assistant":
                return {"response": msg.get("content", "")}

    return {"response": str(result)}


def run_evaluation():
    """Run evaluation with local agent."""
    eval_data = get_eval_data()
    scorers = get_scorers()

    with mlflow.start_run(run_name="eval_run"):
        results = mlflow.genai.evaluate(
            data=eval_data,
            predict_fn=predict_fn,
            scorers=scorers,
        )

    print(f"Run ID: {results.run_id}")
    for metric, value in sorted(results.metrics.items()):
        if metric.endswith('/mean'):
            print(f"  {metric.replace('/mean', '')}: {value:.2%}")

    return results


if __name__ == "__main__":
    run_evaluation()
```

---

## Key Requirements Checklist

Before running the script, verify:

1. **MLflow Setup Order** - `set_tracking_uri()` MUST come before `set_experiment()`
2. **No predict_fn** if dataset has pre-computed `outputs`
3. **predict_fn signature** - Uses `**kwargs` (unpacked), NOT a dict parameter
4. **Use `mlflow.genai.evaluate()`** - NOT `mlflow.evaluate()` (deprecated)
5. **Scorers from correct module** - `from mlflow.genai.scorers import ...`

---

## Common Errors

| Error | Cause | Fix |
|-------|-------|-----|
| `MlflowException: Reading Databricks credential configuration failed` | Missing tracking URI | Add `mlflow.set_tracking_uri("databricks")` before `set_experiment()` |
| `TypeError: predict_fn() got unexpected keyword argument` | Wrong signature | Use `def predict_fn(**kwargs)` or explicit params |
| `ImportError: cannot import name 'evaluate'` | Wrong import | Use `mlflow.genai.evaluate()` |
