# Local Development Environment Configuration
# =============================================
# This file is ONLY for local development (running `uv run python -m src.cli`).
# These variables are NOT used when deploying to Databricks via bundle.
#
# For Databricks deployment:
#   - Configuration is in databricks.yml
#   - Secrets are stored in Databricks secret scope
#   - See README.md "Quick Start (Databricks)" section
#
# For local development:
#   1. Copy this file: cp .env.example .env
#   2. Fill in your values below
#   3. Do NOT commit .env (it's in .gitignore)

# =============================================================================
# DATABRICKS CONFIGURATION (Required: DATABRICKS_HOST)
# =============================================================================

# Databricks workspace host URL (required)
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com

# Authentication: Use either DATABRICKS_TOKEN or DATABRICKS_CONFIG_PROFILE
# DATABRICKS_TOKEN=dapi...

# CLI profile name (alternative to token-based auth)
# DATABRICKS_CONFIG_PROFILE=default

# Serverless compute (enabled by default)
DATABRICKS_SERVERLESS_COMPUTE_ID=auto

# Optional: Specify a cluster ID if not using serverless
# DATABRICKS_CLUSTER_ID=your-cluster-id

# =============================================================================
# MLFLOW CONFIGURATION
# =============================================================================

MLFLOW_TRACKING_URI=databricks

# Target experiment to analyze (the agent being evaluated)
# MLFLOW_EXPERIMENT_ID=123456789

# Experiment for the agent's own traces (for debugging/monitoring the eval agent)
# MLFLOW_AGENT_EXPERIMENT_ID=987654321

# =============================================================================
# MODEL PROVIDER CONFIGURATION
# =============================================================================
# Choose ONE of the options below:

# -----------------------------------------------------------------------------
# Option A: Databricks Foundation Model APIs (Recommended for Databricks users)
# -----------------------------------------------------------------------------
# Route Claude requests through your Databricks workspace's FM API endpoint.
# This uses your Databricks PAT for authentication instead of an Anthropic key.
#
# ANTHROPIC_BASE_URL=https://your-workspace.cloud.databricks.com/serving-endpoints/anthropic
# ANTHROPIC_AUTH_TOKEN=dapi...          # Your Databricks PAT token
# ANTHROPIC_API_KEY=""                  # Must be empty string (required placeholder)
# CLAUDE_CODE_DISABLE_EXPERIMENTAL_BETAS=1  # This must be set in order for Databricks FM API to work properly with Claude Agent SDK!
# MODEL=databricks-claude-opus-4-5      # Model served via Databricks FM API
#
# Note: ANTHROPIC_API_KEY="" is required even though auth uses ANTHROPIC_AUTH_TOKEN.
# The Anthropic SDK requires this env var to be set.

# -----------------------------------------------------------------------------
# Option B: Direct Anthropic API
# -----------------------------------------------------------------------------
# Use Anthropic's API directly with your Anthropic API key.
#
# ANTHROPIC_API_KEY=sk-ant-api03-...
# MODEL=claude-sonnet-4-20250514

# =============================================================================
# PROJECT SETTINGS
# =============================================================================

PROJECT_NAME=mlflow-eval-agent
ENVIRONMENT=development

# =============================================================================
# BENCHMARK CONFIGURATION (for skill_benchmark.py)
# =============================================================================
# These settings are used when running skill benchmark evaluations.
# See: python -m scripts.skill_benchmark --help

# Model for LLM-based scorers (Tier 2 & 3)
# Uses Databricks Foundation Model API endpoint
# Format: databricks:/<model-name>
BENCHMARK_JUDGE_MODEL=databricks:/databricks-gpt-5-2

# MLflow experiment for benchmark results (optional)
# If not set, defaults to skill-benchmarks-<skill-name>
# For Databricks with nested paths, ensure parent folder exists first
# SKILL_BENCHMARK_MLFLOW_EXPERIMENT_NAME=skill-benchmarks-mlflow-evaluation
