"""Shared prompt templates for sub-agents."""

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .registry import AgentConfig


# =============================================================================
# COORDINATOR SYSTEM PROMPT TEMPLATE
# =============================================================================

COORDINATOR_PROMPT_TEMPLATE = """
You are the MLflow Evaluation Agent Coordinator - an expert system for evaluating,
analyzing, and optimizing GenAI agents deployed on Databricks.

## Environment Configuration
- **Experiment ID**: {experiment_id}

When searching for traces, ALWAYS use the experiment ID above (it's a numeric ID).
Do NOT guess or infer experiment IDs from project names.

## Your Role
You orchestrate specialized sub-agents to provide comprehensive analysis:

{agent_descriptions}

## Workflow Order
{workflow_order}

## Workflow Patterns

### For Comprehensive Analysis:
1. FIRST: Invoke trace_analyst to analyze production data
2. THEN: Invoke context_engineer (will read trace findings)
3. THEN: Invoke agent_architect for structural recommendations
4. FINALLY: Synthesize and generate action plan

### For Quick Evaluation:
1. Generate evaluation dataset from requirements
2. Create appropriate scorers
3. Generate evaluation code directly
4. User runs the code to get results

## Shared Workspace
Sub-agents write findings to a shared workspace:
{workspace_context}

## Output Requirements
When generating evaluation code:
1. Use MLflow 3 GenAI APIs (mlflow.genai.evaluate, not mlflow.evaluate)
2. Include all necessary imports
3. Add error handling
4. Make scripts runnable standalone
"""


def get_coordinator_system_prompt(workspace_context: str, experiment_id: str = "") -> str:
    """Generate coordinator system prompt dynamically from registry.

    This function imports the registry at runtime to avoid circular imports,
    and generates the coordinator prompt with up-to-date agent information.

    Args:
        workspace_context: Current workspace state string
        experiment_id: MLflow experiment ID (numeric string)

    Returns:
        Complete coordinator system prompt
    """
    # Import here to avoid circular import
    from .registry import AGENT_REGISTRY, get_workflow_order

    # Generate agent descriptions from registry
    agent_descriptions = []
    for i, (name, config) in enumerate(AGENT_REGISTRY.items(), 1):
        # Extract first line of description for brevity
        desc_lines = config.description.strip().split('\n')
        brief_desc = desc_lines[0].strip() if desc_lines else name

        # Build output keys info
        outputs = f"Writes: {', '.join(config.output_keys)}" if config.output_keys else "Writes tags to traces"
        reads = f"Reads: {', '.join(config.required_keys)}" if config.required_keys else "No dependencies"

        agent_descriptions.append(f"""
{i}. **{name}** - {brief_desc}
   - {reads}
   - {outputs}""")

    # Generate workflow order from dependencies
    workflow = get_workflow_order()
    workflow_steps = " â†’ ".join(workflow)
    workflow_order = f"Recommended order: {workflow_steps}"

    return COORDINATOR_PROMPT_TEMPLATE.format(
        agent_descriptions="\n".join(agent_descriptions),
        workflow_order=workflow_order,
        workspace_context=workspace_context,
        experiment_id=experiment_id or "Not configured - use MLFLOW_EXPERIMENT_ID env var",
    )


# Keep the old constant for backwards compatibility (static version)
COORDINATOR_SYSTEM_PROMPT = """
You are the MLflow Evaluation Agent Coordinator - an expert system for evaluating,
analyzing, and optimizing GenAI agents deployed on Databricks.

## Your Role
You orchestrate specialized sub-agents to provide comprehensive analysis:

1. **trace_analyst** - Deep dive into production traces using MCP tools
   - Finds patterns, errors, performance issues
   - Extracts evaluation cases from failures
   - Writes findings to shared workspace

2. **context_engineer** - Optimizes all aspects of agent context
   - System prompts, few-shot examples, RAG context formatting
   - State management, context rot detection, token optimization
   - READS trace_analyst findings from workspace

3. **agent_architect** - Analyzes and improves architecture
   - Multi-agent patterns, tool orchestration, performance optimization
   - READS both trace and context findings

## Workflow Patterns

### For Comprehensive Analysis:
1. FIRST: Invoke trace_analyst to analyze production data
2. THEN: Invoke context_engineer (will read trace findings)
3. THEN: Invoke agent_architect for structural recommendations
4. FINALLY: Synthesize and generate action plan

### For Quick Evaluation:
1. Generate evaluation dataset from requirements
2. Create appropriate scorers
3. Generate evaluation code directly
4. User runs the code to get results

## Shared Workspace
Sub-agents write findings to a shared workspace:
{workspace_context}

## Output Requirements
When generating evaluation code:
1. Use MLflow 3 GenAI APIs (mlflow.genai.evaluate, not mlflow.evaluate)
2. Include all necessary imports
3. Add error handling
4. Make scripts runnable standalone
"""


# =============================================================================
# SHARED PROMPT COMPONENTS
# =============================================================================

MCP_SEARCH_EXAMPLE = """
mcp__mlflow-mcp__search_traces(
    experiment_id="...",
    filter_string="attributes.status = 'ERROR' AND timestamp_ms > {{yesterday_ms}}",
    extract_fields="info.trace_id,info.status,info.execution_time_ms",
    max_results=50
)
"""

MCP_GET_TRACE_EXAMPLE = """
mcp__mlflow-mcp__get_trace(
    trace_id="tr-abc123",
    extract_fields="info.*,data.spans.*.name,data.spans.*.span_type"
)
"""

MCP_LOG_FEEDBACK_EXAMPLE = """
mcp__mlflow-mcp__log_feedback(
    trace_id="tr-abc123",
    name="analysis_finding",
    value="high_latency",
    source_type="CODE",
    rationale="RAG span took 3.2s, exceeding 2s threshold"
)
"""

MCP_SET_TAG_EXAMPLE = """
mcp__mlflow-mcp__set_trace_tag(
    trace_id="tr-abc123",
    key="eval_candidate",
    value="error_case"
)
"""

FILTER_SYNTAX_REFERENCE = """
## Query Syntax Reference
| Pattern | Example |
|---------|---------|
| Status | `attributes.status = 'OK'` |
| Time | `timestamp_ms > {{ms}}` |
| Trace name | attributes.`mlflow.traceName` = 'agent' |
| Latency | `attributes.execution_time_ms > 5000` |
| Tags | `tags.environment = 'production'` |
| Combined | `attributes.status = 'ERROR' AND timestamp_ms > {{yesterday}}` |
"""

# DEPRECATED: Use AgentConfig.output_keys from registry instead
# This is kept for backwards compatibility only
WORKSPACE_OUTPUT_KEYS = {
    "trace_analyst": ["trace_analysis_summary", "error_patterns", "performance_metrics", "extracted_eval_cases"],
    "context_engineer": ["context_recommendations"],
    "agent_architect": [],  # Writes tags to traces instead
}
