# Token Tracking in MLflow Traces

This document explains the token tracking implementation and how to interpret token metrics in MLflow traces.

## Implementation

Token tracking was added to `src/agent.py` to capture usage data from the Claude Agent SDK and set it as span attributes on MLflow traces.

### Attributes Captured

| Attribute | Description |
|-----------|-------------|
| `input_tokens` | New/uncached tokens in the prompt |
| `output_tokens` | Tokens generated by the model |
| `cache_creation_input_tokens` | Tokens written to cache (25% surcharge) |
| `cache_read_input_tokens` | Tokens read from cache (90% discount) |
| `total_tokens` | `input_tokens + output_tokens` |
| `cost_usd` | Total cost in USD |

### Where Attributes Appear

1. **`agent_query` span**: Inner span from the `query()` method
2. **`session_N` span**: Outer iteration span in autonomous mode

Both levels have identical token data for easy querying.

---

## Understanding Token Types

### Input Tokens (`input_tokens`)

Tokens in the **new/uncached** portion of your prompt. These are tokens that:
- Weren't previously cached
- Changed since the last request
- Are in positions that invalidate the cache prefix

Typically very low (single digits) when prompt caching is effective.

### Output Tokens (`output_tokens`)

Tokens the model generates in its response. Always billed at full rate. Includes:
- Reasoning and explanations
- Generated code
- Tool call arguments

### Cache Read Tokens (`cache_read_input_tokens`)

Tokens read from Claude's prompt cache at a **90% discount**. This includes:
- System prompt
- Tool definitions
- Stable conversation history
- Any content that matches the cached prefix

**This is where you save money.**

### Cache Creation Tokens (`cache_creation_input_tokens`)

Tokens written to cache for future use at a **25% surcharge**. This is an investment:
- First request pays to cache the prompt
- Subsequent requests benefit from cache reads

---

## The Token Relationship

```
Total Context = input_tokens + cache_read_input_tokens + cache_creation_input_tokens
```

### Example (from a real session):

```
Session 1:
  input_tokens:                    8
  cache_read_input_tokens:   802,667
  cache_creation_input_tokens: 87,136
  ─────────────────────────────────
  Total Context:             889,811 tokens
```

**What's happening:**
- 8 tokens are truly new (the user's query)
- 802,667 tokens were read from cache (system prompt, tools, prior context)
- 87,136 tokens were added to cache (new content for future requests)

---

## Cost Implications

Claude's pricing (Opus, approximate):

| Token Type | Price per 1M | Multiplier |
|------------|--------------|------------|
| Input | $15 | 1x |
| Output | $75 | 5x |
| Cache Read | $1.50 | 0.1x (90% off!) |
| Cache Create | $18.75 | 1.25x |

### Example Cost Breakdown

```
Session with 800K cache reads vs no caching:

With Cache:
  Cache read: 800,000 × $1.50/M = $1.20

Without Cache:
  Input: 800,000 × $15/M = $12.00

Savings: $10.80 (90%)
```

---

## Interpreting Trace Data

### Healthy Patterns

1. **High cache_read, low input_tokens**: Good cache utilization
2. **Decreasing cache_creation over time**: Settling into steady state
3. **Stable cache_read across sessions**: Consistent context size

### Warning Signs

1. **High cache_creation every session**: Prompt instability invalidating cache
2. **Spiking input_tokens**: Dynamic content that can't be cached
3. **Decreasing cache_read**: Context being truncated or changed

### Example Analysis

```
Session 1: cache_create=87K, cache_read=802K  (initializing)
Session 2: cache_create=95K, cache_read=767K  (adding context)
Session 3: cache_create=36K, cache_read=681K  (stabilizing)
Session 4: cache_create=33K, cache_read=404K  (context shrinking?)
Session 5: cache_create=23K, cache_read=308K  (steady state)
```

**Observations:**
- Cache creation decreases over time (good)
- Cache read also decreases (investigate - context may be getting truncated)

---

## Querying Token Data

### Using MLflow Python API

```python
import mlflow

trace = mlflow.get_trace(trace_id)
for span in trace.data.spans:
    if span.name.startswith("session_"):
        attrs = span.attributes or {}
        print(f"{span.name}:")
        print(f"  input_tokens: {attrs.get('input_tokens', 0)}")
        print(f"  output_tokens: {attrs.get('output_tokens', 0)}")
        print(f"  cache_read: {attrs.get('cache_read_input_tokens', 0)}")
        print(f"  cache_create: {attrs.get('cache_creation_input_tokens', 0)}")
        print(f"  cost: ${attrs.get('cost_usd', 0):.4f}")
```

### Using MCP Tools

```bash
mlflow traces get --trace-id <id> \
  --extract-fields "data.spans.*.name,data.spans.*.attributes"
```

---

## Optimization Tips

1. **Stabilize your system prompt**: Changes invalidate the cache prefix

2. **Put static content first**: Cache works on prefixes, so stable content at the start maximizes cache hits

3. **Monitor cache_creation**: If it stays high across sessions, investigate what's changing

4. **Watch for input_token spikes**: Indicates dynamic content that can't benefit from caching - consider restructuring

5. **Use the analysis script**:
   ```bash
   python scripts/analyze_trace.py <trace_id> --full
   ```

---

## References

- [Claude Prompt Caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)
- [Claude Agent SDK Cost Tracking](https://platform.claude.com/docs/en/agent-sdk/cost-tracking)
- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)
